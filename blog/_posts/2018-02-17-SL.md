---
layout: single
title: "Supervised Training for Fake Hotel Review"
date: 2018-02-17
---

# Hotel Reviews: What is real?
## Problem Statement 
When someone goes on to plan a stay, they tend to use online reviews to base their decision; However, there are usually a range of opinions so how can we tell if a certain review is fake/real or just plain bad luck? Thus the problem statement is to detect fradulent reviews for people are surfing hotel booking sites. 

## Goals 
When someone goes on to plan a stay, they tend to use online reviews to base their decision; However, there are usually a range of opinions so how can we tell if a certain review is fake/real or just plain bad luck? Thus the problem statement is to detect fradulent reviews for people are surfing hotel booking sites. 

The end-goal would be to have a website where users can copy and paste a hotel review and a result would be returned - either a probability that the reivew is fake/true or a color-coded general response if the review was trustworthy. 

Furthermore, there is a core labelled corpus of 1600 hotel reviews, and 512k 
unlabelled hotel reviews. I would like to utilize these unlabelled reviews in a semi-supervised fashion. 

## Metrics
Given our end-goal in mind, I felt that users would appreciate precision more; if a high number of reviews they put into the review app returned false, they might get turned off by the high amount of false positives and it does not aid their selection process. They might even not trust the system at all. Therefore F1 or precision score would be the metrics. We aim to hit 71% to 74% precision with good f1 score. 

## Risks/Limitations/Assumptions
### *Many language libraries black boxes*
There are quite many NLP libraries available (NLTK, gensim, etc.) which means we have to try substanial number of libraries to assess their results.

Each run of the algorithm takes up a significant amount of time, thus more time is needed so a CUDA-specific libray (TensorFlow) could be used.However this would mean even more time spent on on-boarding. 

There is a assumption that there actually exists some pattern or trend that exists in both truthful and fake reviews. 
### *Data Issues & Assumptions*
There is an assumption that there are common trends between labelled and unlabelled data that would show up and thus allow us to cluster them and potentially improve the supervised training. However this may not be, as one collection was generated mostly by MTurk and TripAdvisor and one was collected from European hotels, potentially with just some overlap. 

After generating all the vectors from doc2vec, lda, and other algorithms, the feature vector is of a very high dimension, and not easy to flatten into 2-D space for visualization (PCA did not produce features with a lot of energy), limiting the intuition for the data. Also, the model needed all of the features to obtain the best supervised result. 

