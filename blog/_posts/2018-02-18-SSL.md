---
layout: single
title: "Fake Hotel Review-4"
date: 2018-02-18
---

# Semi-Supervised Training
Next, I moved on to semi-supervised training. The idea is that since I have a large collection of unlabelled datasets, that it could be possible to train a better model by using the unlabelled dataset in a semi-supervised fashion. Since semi-supervised training is a area of active research right now, this portion is more of a exercise in discovery. I explored several different SSL techniques. Before I started, I randomly subsampled 30% of the 512k unlabelled records as the unlabelled training set. 

## Pseduo Labelling 
Using this [Source](https://datawhatnow.com/pseudo-labeling-semi-supervised-learning/), where we label unlabelled data to have pseduo-labeled data, and then we reuse both labelled and pseudo-labelled data to retrain a new model. 
![Pseudo-Labelling Process](https://i2.wp.com/datawhatnow.com/wp-content/uploads/2017/08/pseudo-labeling.png?resize=683%2C1024&ssl=1)
I used the best model learned from the supervised training (GridSearch XGBoost).

## SKLearn Label Propagation
```scikit-learn``` has its own semi-supervised training algorithms, called [Label Propagation and Label Spreading](http://scikit-learn.org/stable/modules/label_propagation.html). In theory, the model would be able to find connected points in the space and cluster them appropriately. 

![Label Propagation](http://scikit-learn.org/stable/_images/sphx_glr_plot_label_propagation_structure_0011.png)

\In the target vector, values of -1 have to given to unlabelled feature rows. Then we call the ```fit_predict``` paradigm of scikit-learn. 

```python
from sklearn.semi_supervised import label_propagation
from scipy.sparse import csgraph

#Label Propagation
label_prop = label_propagation.LabelPropagation(kernel='rbf', gamma=0.2, n_jobs=-1)
label_prop.fit(X_train_new, y_train_new)
best_pred = label_prop.predict(X_test)
print(classification_report(y_test, best_pred))
```
The main issue of this algorithm is that it uses a lot of memory thus it would easily be a problem for large datasets. 

## LocalitySensitive Hashing
[Locality Sensitive Hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) is the process where high dimensionality data is hashed into similar space with high probability. Thus it is different from normal hashing function where the aim is to minimise the probability of collisison of similar items. I used the [source code](https://engineering.purdue.edu/kak/distLSH/LocalitySensitiveHashing-1.0.1.html) developed by Avinash Kak. As described, its to avoid having to perform many nearest neighbor similarity comparison which will be computationally expensive. It must also be noted that the similarity obtained through LSH is not transitive: If Sample X is Sample Y's nearest neighbour and Y is Sample Z's nearest neighbour, it does not always imply that X and Z are sufficiently close enough to be nearest neighbours as well. 

```python
lsh = LocalitySensitiveHashing(datafile='./labelled_corpus.csv', # must be a csv file
                              dim = 398, # number of features in data set
                              r = 50, # number of rows in 1 band of hash functions
                              b = 100, # number of r-row bands of hash functions
                              expected_num_of_clusters = 8)
lsh.get_data_from_csv()
lsh.initialize_hash_store()
lsh.hash_all_data()
similarity_groups = lsh.lsh_basic_for_neighborhood_clusters()
coalesced_similarity_groups = lsh.merge_similarity_groups_with_coalescence( similarity_groups )
merged_similarity_groups = lsh.merge_similarity_groups_with_l2norm_sample_based( coalesced_similarity_groups )
lsh.write_clusters_to_file( merged_similarity_groups, "clusters.txt" )
```
This is a fully unsupervised method, thus later we compared the clustering against the ground truth and check if there is any correlation between the ground truth and clustering gernerated by LSH. If there is, perhaps we can find the significant correlation in common features in both. 

## Recursive KMeans
Kmeans clustering is a unsupervised method; [Semi-Supervised Text Categorization](https://arxiv.org/ftp/arxiv/papers/1706/1706.07913.pdf)utilizes recursive KMeans clustering. This is perfect given our situation: a small number of labelled samples and a large amount of unlabelled samples. First given the labelled datasets, the exact number of unique clusters are given. An initial KMeans clustering is performed by first randomly selecting a point of each unique cluster label, and performing KMeans clustering over both labelled and unlabelled samples. Then for each resulting cluster that is formed, if all the labelled points within the cluster are of the same type, then the cluster is deemed correctly labelled. If the labelled points within the cluster have multiple distinct labels, then Kmeans is recursively applied to the cluster until all the labels have only 1 distinct label on them. Then there would be multiple clusters each with 1 distinct label each: this would be the label of each individual cluster. To predict a new sample, the sample would have the same label as the cluster with the closest centroid to the sample. I reimplemented (but not equivalent to the algorithm in the paper) using a class with a similar ```fit_predict``` paradigm as ```sci-kit``` classifiers.

```python
from sklearn.cluster import KMeans
class RKmeansSSL:
    def __init__(self):
        print('Init')
        
    def initSeed(self,X_train, y_train, y_colname):
        self.k = y_train[y_colname].unique().shape[0]
        self.unique_val = y_train[y_colname].unique()
        self.initialSeedIndice=[]
        self.newCentroids = []
        self.newCentClass = []
        # for each class, pick a random element as seed
        for u in self.unique_val:
            self.initialSeedIndice.append(y_train[y_train[y_colname]==u].sample(n=1).index[0])
        
    
    def fit(self,X_train, y_train, y_colname, unlabelled, threshold=1):
        self.initSeed(X_train,y_train,y_colname)
        # with initial seeds, run initial Cluster
        km = KMeans(n_clusters=self.k, init=X_train.iloc[self.initialSeedIndice])
        X_train2 = pd.concat([X_train, unlabelled], axis=0)
        km.fit(X_train2)
        self.checkCluster(km, X_train2,y_train[y_colname],y_train.shape[0])
        self.knn = KNeighborsClassifier(n_neighbors=1)
        self.knn.fit(self.newCentroids, self.newCentClass)
    
    def predict(self, X_test):
        return self.knn.predict(X_test)
    
    # Within each group, check against the labelled data if there are any 
    # mislabelled rows. If so, recurse
    def checkCluster(self, km, X_train, y_train, buffer):
        new_labels = pd.Series(km.labels_).iloc[range(buffer)]
        for u in self.unique_val:
            # find the mislabelled points within the buffer length
            match_ind = new_labels[new_labels==u].index
            if (y_train.iloc[match_ind].unique().shape[0]>1):
                self.recurseCluster(X_train[km.labels_==u], y_train.iloc[match_ind],len(y_train.iloc[match_ind]))
            else:
                # we found the whole cluster of points has a single label, therefore we save it as a new centroid
                print("Centroid Found")
                self.newCentroids.append(X_train[km.labels_==u].mean())
                self.newCentClass.append(u)
    
    def recurseCluster(self, X_train, y_train, buffer):
        km = KMeans(n_clusters=self.k)
        km.fit(X_train)
        self.checkCluster(km, X_train,y_train,len(y_train))

rk = RKmeansSSL()
# The first argument is the labelled sample
# The second argument is the labelled sample target label DataFrame
# The third argument is the name of the target column in target_train
# The fourth argument is the unlabelled samples
rk.fit(data_train,target_train,'deceptive',rand_unlabelled_vec)
```

## Contrastive Pessimistic Likelihood Estimation
I also took a look at another [Semi-supervised learning framework](https://github.com/tmadl/semisup-learn). 2 Points to note about the library: Firstly it is written to work in Python 2.7 environment, so please use a Python 2.7 environment! (This caused a day or 2 of pain.) Secondly, it requires NLopt to run, and there are some issues with it in a Windows environment. I would recommend downloading the correct binary for Windows OS/Python Version from [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#nlopt). 




## DBScan 