---
layout: single
title: "Fake Hotel Review-4"
date: 2018-02-18
---

# Semi-Supervised Training
Next, I moved on to semi-supervised training. The idea is that since I have a large collection of unlabelled datasets, that it could be possible to train a better model by using the unlabelled dataset in a semi-supervised fashion. Since semi-supervised training is a area of active research right now, this portion is more of a exercise in discovery. I explored several different SSL techniques. Before I started, I randomly subsampled 30% of the 512k unlabelled records as the unlabelled training set. 

## Pseduo Labelling 
Using this [Source](https://datawhatnow.com/pseudo-labeling-semi-supervised-learning/), where we label unlabelled data to have pseduo-labeled data, and then we reuse both labelled and pseudo-labelled data to retrain a new model. 
![Pseudo-Labelling Process](https://i2.wp.com/datawhatnow.com/wp-content/uploads/2017/08/pseudo-labeling.png?resize=683%2C1024&ssl=1)
I used the best model learned from the supervised training (GridSearch XGBoost).

## SKLearn Label Propagation
```scikit-learn``` has its own semi-supervised training algorithms, called [Label Propagation and Label Spreading](http://scikit-learn.org/stable/modules/label_propagation.html). In theory, the model would be able to find connected points in the space and cluster them appropriately. 

![Label Propagation](http://scikit-learn.org/stable/_images/sphx_glr_plot_label_propagation_structure_0011.png)

In the target vector, values of -1 have to given to unlabelled feature rows. Then we call the ```fit_predict``` paradigm of scikit-learn. 

```python
from sklearn.semi_supervised import label_propagation
from scipy.sparse import csgraph

#Label Propagation
label_spread = label_propagation.LabelSpreading(kernel='rbf', alpha=0.1, gamma=0.2, n_jobs=-1)
label_spread.fit(X_train_new, y_train_new)
best_pred = label_spread.predict(X_test)
print(classification_report(y_test, best_pred))
```