---
layout: single
title: "Fake Hotel Review-4"
date: 2018-02-18
---

# Semi-Supervised Training
Next, I moved on to semi-supervised training. The idea is that since I have a large collection of unlabelled datasets, that it could be possible to train a better model by using the unlabelled dataset in a semi-supervised fashion. Since semi-supervised training is a area of active research right now, this portion is more of a exercise in discovery. I explored several different SSL techniques. Before I started, I randomly subsampled 30% of the 512k unlabelled records as the unlabelled training set. 

## Pseduo Labelling 
Using this [Source](https://datawhatnow.com/pseudo-labeling-semi-supervised-learning/), where we label unlabelled data to have pseduo-labeled data, and then we reuse both labelled and pseudo-labelled data to retrain a new model. 
![Pseudo-Labelling Process](https://i2.wp.com/datawhatnow.com/wp-content/uploads/2017/08/pseudo-labeling.png?resize=683%2C1024&ssl=1)
I used the best model learned from the supervised training (GridSearch XGBoost).

## SKLearn Label Propagation
```scikit-learn``` has its own semi-supervised training algorithms, called [Label Propagation and Label Spreading](http://scikit-learn.org/stable/modules/label_propagation.html). In theory, the model would be able to find connected points in the space and cluster them appropriately. 

![Label Propagation](http://scikit-learn.org/stable/_images/sphx_glr_plot_label_propagation_structure_0011.png)

\In the target vector, values of -1 have to given to unlabelled feature rows. Then we call the ```fit_predict``` paradigm of scikit-learn. 

```python
from sklearn.semi_supervised import label_propagation
from scipy.sparse import csgraph

#Label Propagation
label_prop = label_propagation.LabelPropagation(kernel='rbf', gamma=0.2, n_jobs=-1)
label_prop.fit(X_train_new, y_train_new)
best_pred = label_prop.predict(X_test)
print(classification_report(y_test, best_pred))
```


## LocalitySensitive Hashing
[Locality Sensitive Hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) is the process where high dimensionality data is hashed into similar space with high probability. Thus it is different from normal hashing function where the aim is to minimise the probability of collisison of similar items. I used the source code developed by [Avinash](https://engineering.purdue.edu/kak/distLSH/LocalitySensitiveHashing-1.0.1.html). As described, its to avoid having to perform many nearest neighbor similarity comparison which will be computationally expensive. It must also be noted that the similarity obtained through LSH is not transitive: If Sample X is Sample Y's nearest neighbour and Y is Sample Z's nearest neighbour, it does not always implty that X and Z are sufficiently close enough to be nearest neighbours as well. 

```python
lsh = LocalitySensitiveHashing(datafile='./labelled_corpus.csv', # must be a csv file
                              dim = 398, # number of features in data set
                              r = 50, # number of rows in 1 band of hash functions
                              b = 100, # number of r-row bands of hash functions
                              expected_num_of_clusters = 8)
```
## Recursive KMeans

## Contrastive Pessimistic Likelihood Estimation





## DBScan 